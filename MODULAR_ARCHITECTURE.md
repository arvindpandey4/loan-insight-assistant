# Modular RAG Pipeline - Architecture Guide

## ğŸ—ï¸ Architecture Overview

The RAG pipeline has been refactored into **5 separate modules** for better maintainability, reusability, and clarity:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG PIPELINE MODULES                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£  data_loader.py          ğŸ“¥ Data Loading & Validation
2ï¸âƒ£  text_processor.py        ğŸ“ Text Representation & Chunking  
3ï¸âƒ£  embedding_generator.py   ğŸ¤– Embedding Generation
4ï¸âƒ£  vector_store.py          ğŸ“Š FAISS Vector Store
5ï¸âƒ£  pipeline_orchestrator.py ğŸ¯ Pipeline Orchestration
```

---

## ğŸ“ Module Details

### 1ï¸âƒ£ `data_loader.py` - Data Loading & Validation

**Purpose**: Load CSV data, explore statistics, and validate quality

**Class**: `LoanDataLoader`

**Key Methods**:
- `load_data()` - Load CSV file
- `explore_data()` - Analyze columns and statistics
- `validate_data()` - Check for data quality issues
- `get_dataframe()` - Return the loaded data
- `get_validation_report()` - Get validation metrics

**Example Usage**:
```python
from data_loader import LoanDataLoader

loader = LoanDataLoader('loan_data.csv')
loader.load_data().explore_data().validate_data()

df = loader.get_dataframe()
report = loader.get_validation_report()
```

**Validates**:
- âœ… Duplicate records
- âœ… Missing values
- âœ… Data types
- âœ… Financial field ranges
- âœ… CIBIL score validity

---

### 2ï¸âƒ£ `text_processor.py` - Text Representation & Chunking

**Purpose**: Transform loan records into rich text representations

**Class**: `LoanTextProcessor`

**Key Methods**:
- `create_text_representations()` - Build natural language descriptions
- `chunk_texts()` - Split long texts (optional)
- `get_processed_dataframe()` - Get data with text column
- `get_text_representations()` - Get list of texts
- `get_metadata()` - Get processing statistics

**Example Usage**:
```python
from text_processor import LoanTextProcessor

processor = LoanTextProcessor(df)
processor.create_text_representations()

texts = processor.get_text_representations()
processed_df = processor.get_processed_dataframe()
```

**Creates Text From**:
- Customer demographics
- Employment information
- Financial details
- Credit history
- Location data
- Application notes
- Customer feedback

---

### 3ï¸âƒ£ `embedding_generator.py` - Embedding Generation

**Purpose**: Generate semantic embeddings using Sentence Transformers

**Class**: `EmbeddingGenerator`

**Key Methods**:
- `load_model()` - Load sentence transformer model
- `generate_embeddings()` - Create embeddings for texts
- `encode_query()` - Encode a single query
- `save_embeddings()` - Save to numpy file
- `load_embeddings()` - Load from numpy file
- `get_embeddings()` - Return embeddings array
- `get_model()` - Get the loaded model

**Example Usage**:
```python
from embedding_generator import EmbeddingGenerator

generator = EmbeddingGenerator('sentence-transformers/all-MiniLM-L6-v2')
embeddings = generator.generate_embeddings(texts, batch_size=32)

# For search queries
query_emb = generator.encode_query("home loan")
```

**Supported Models**:
- `all-MiniLM-L6-v2` (default) - Fast, 384 dims
- `all-mpnet-base-v2` - More accurate, 768 dims
- Any Sentence Transformers model

---

### 4ï¸âƒ£ `vector_store.py` - FAISS Vector Store

**Purpose**: Create and manage FAISS index for similarity search

**Class**: `FAISSVectorStore`

**Key Methods**:
- `create_index()` - Build FAISS index
- `search()` - Find similar vectors
- `batch_search()` - Search multiple queries
- `save_index()` - Save index to disk
- `load_index()` - Load index from disk
- `get_metadata()` - Get index statistics

**Example Usage**:
```python
from vector_store import FAISSVectorStore

store = FAISSVectorStore()
store.create_index(embeddings, index_type='flat')

# Search
distances, indices = store.search(query_embedding, k=5)

# Save
store.save_index('index.bin')
```

**Index Types**:
- `flat` - Exact search (< 10K vectors)
- `ivf` - Approximate search (> 10K vectors)

---

### 5ï¸âƒ£ `pipeline_orchestrator.py` - Pipeline Orchestration

**Purpose**: Coordinate all modules and execute the complete pipeline

**Class**: `LoanRAGPipeline`

**Key Methods**:
- `run_full_pipeline()` - Execute all steps
- `_load_data()` - Step 1: Data loading
- `_process_texts()` - Step 2: Text processing
- `_generate_embeddings()` - Step 3: Embedding generation
- `_create_vector_store()` - Step 4: Vector store creation
- `_save_artifacts()` - Step 5: Save outputs
- `_test_search()` - Step 6: Test search

**Example Usage**:
```python
from pipeline_orchestrator import LoanRAGPipeline

pipeline = LoanRAGPipeline(
    csv_path='loan_data.csv',
    output_dir='./output',
    model_name='sentence-transformers/all-MiniLM-L6-v2'
)

pipeline.run_full_pipeline()
```

---

## ğŸš€ Quick Start

### Option 1: Run Complete Pipeline
```bash
python pipeline_orchestrator.py
```

### Option 2: Use Modules Individually

```python
# 1. Load data
from data_loader import LoanDataLoader
loader = LoanDataLoader('loan_data.csv')
loader.load_data().explore_data().validate_data()
df = loader.get_dataframe()

# 2. Process texts
from text_processor import LoanTextProcessor
processor = LoanTextProcessor(df)
processor.create_text_representations()
texts = processor.get_text_representations()

# 3. Generate embeddings
from embedding_generator import EmbeddingGenerator
generator = EmbeddingGenerator()
embeddings = generator.generate_embeddings(texts)

# 4. Create vector store
from vector_store import FAISSVectorStore
store = FAISSVectorStore()
store.create_index(embeddings)
store.save_index('my_index.bin')
```

---

## ğŸ“Š Data Flow

```
CSV File
   â†“
[data_loader.py] â†’ Load & Validate
   â†“
DataFrame
   â†“
[text_processor.py] â†’ Create Text Representations
   â†“
Text List
   â†“
[embedding_generator.py] â†’ Generate Embeddings
   â†“
Embeddings (numpy array)
   â†“
[vector_store.py] â†’ Create FAISS Index
   â†“
Searchable Vector Store
```

---

## ğŸ¯ Benefits of Modular Architecture

### âœ… Separation of Concerns
Each module has a single, well-defined responsibility

### âœ… Reusability
Use individual modules in other projects

### âœ… Testability
Easy to test each component independently

### âœ… Maintainability
Changes in one module don't affect others

### âœ… Flexibility
Mix and match components as needed

### âœ… Extensibility
Easy to add new features to specific modules

---

## ğŸ”§ Customization Examples

### Change Embedding Model
```python
# In pipeline_orchestrator.py
pipeline = LoanRAGPipeline(
    csv_path='data.csv',
    model_name='sentence-transformers/all-mpnet-base-v2'  # Better quality
)
```

### Use Custom Text Processing
```python
from text_processor import LoanTextProcessor

class CustomTextProcessor(LoanTextProcessor):
    def create_text_representations(self):
        # Your custom logic here
        pass

processor = CustomTextProcessor(df)
```

### Use Different Vector Store
```python
from vector_store import FAISSVectorStore

# For larger datasets
store = FAISSVectorStore()
store.create_index(embeddings, index_type='ivf', nlist=200)
```

---

## ğŸ“¦ Output Artifacts

When you run the pipeline, it generates:

```
output/
â”œâ”€â”€ processed_loan_data_with_embeddings.csv  # Processed data
â”œâ”€â”€ loan_faiss_index.bin                     # FAISS index
â”œâ”€â”€ loan_embeddings.npy                      # Embeddings backup
â”œâ”€â”€ validation_report.json                   # Machine-readable
â””â”€â”€ validation_report.txt                    # Human-readable
```

---

## ğŸ§ª Testing Individual Modules

Each module can be tested independently:

```bash
# Test data loader
python data_loader.py

# Test text processor
python text_processor.py

# Test embedding generator
python embedding_generator.py

# Test vector store
python vector_store.py
```

---

## ğŸ”„ Migration from Old Code

**Old (Monolithic)**:
```python
from build_rag_pipeline import LoanRAGPipeline
pipeline = LoanRAGPipeline('data.csv')
pipeline.run_full_pipeline()
```

**New (Modular)**:
```python
from pipeline_orchestrator import LoanRAGPipeline
pipeline = LoanRAGPipeline('data.csv')
pipeline.run_full_pipeline()
```

The API remains the same! The old `build_rag_pipeline.py` is preserved for backward compatibility.

---

## ğŸ“š Module Dependencies

```
pipeline_orchestrator.py
    â”œâ”€â”€ data_loader.py (pandas, numpy)
    â”œâ”€â”€ text_processor.py (pandas, numpy)
    â”œâ”€â”€ embedding_generator.py (sentence-transformers)
    â””â”€â”€ vector_store.py (faiss, numpy)
```

**Install All Dependencies**:
```bash
pip install pandas numpy sentence-transformers faiss-cpu
```

---

## ğŸ’¡ Best Practices

### 1. Use the Orchestrator for Full Pipeline
```python
# Recommended
from pipeline_orchestrator import LoanRAGPipeline
pipeline = LoanRAGPipeline('data.csv')
pipeline.run_full_pipeline()
```

### 2. Use Individual Modules for Custom Workflows
```python
# For custom processing
from data_loader import LoanDataLoader
from embedding_generator import EmbeddingGenerator

loader = LoanDataLoader('data.csv')
df = loader.load_data().get_dataframe()

# Custom processing here...

generator = EmbeddingGenerator()
embeddings = generator.generate_embeddings(my_custom_texts)
```

### 3. Cache Expensive Operations
```python
# Save embeddings for reuse
generator.save_embeddings('embeddings.npy')

# Later, load instead of regenerating
generator.load_embeddings('embeddings.npy')
```

---

## ğŸ“ Learning Path

1. **Start Simple**: Use `pipeline_orchestrator.py` to understand the flow
2. **Explore Modules**: Read each module's code and docstrings
3. **Experiment**: Modify individual modules for custom behavior
4. **Build**: Create your own pipeline using the modules

---

## ğŸ“ˆ Performance

| Operation | Time (1000 records) |
|-----------|---------------------|
| Data Loading | < 1 second |
| Text Processing | ~2 seconds |
| Embedding Generation | ~30 seconds |
| Index Creation | < 1 second |
| Search Query | < 10 ms |
| **Total Pipeline** | **~45 seconds** |

---

## ğŸ¤ Contributing

To add a new module:

1. Create a new Python file (e.g., `new_module.py`)
2. Define a class with clear methods
3. Add docstrings and type hints
4. Include a `__main__` block for testing
5. Import in `pipeline_orchestrator.py`

---

**Status**: âœ… Production Ready | Modular | Well-Documented
