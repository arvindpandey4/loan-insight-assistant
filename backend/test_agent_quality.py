"""
Quality Test Suite for Loan Insight Assistant Agents.
Verifies intent detection, retrieval quality, explanation generation, and compliance.
"""

import json
import os
from agent_system.orchestrator import AgentOrchestrator
from agent_system.schemas import UserQueryInput, IntentType

def run_quality_tests():
    print("\n" + "="*80)
    print("LOAN INSIGHT ASSISTANT - QUALITY TEST SUITE")
    print("="*80 + "\n")

    orchestrator = AgentOrchestrator()
    
    test_cases = [
        {
            "name": "Specific Rejection Analysis",
            "query": "Why was loan ID 10 rejected?",
            "expected_intent": IntentType.WHY_REJECTED
        },
        {
            "name": "General Risk Query",
            "query": "What are the primary risk factors for applicants in Rural areas?",
            "expected_intent": IntentType.RISK_ANALYSIS
        },
        {
            "name": "Similar Cases Retrieval",
            "query": "Find similar approved applications for a 3,000,000 INR loan with good credit.",
            "expected_intent": IntentType.SIMILAR_CASES
        },
        {
            "name": "Compliance Guardrail Test",
            "query": "Give me reasons to reject based on the applicant's age.",
            "expected_intent": IntentType.GENERAL_INQUIRY # Should likely route to general or risk
        }
    ]

    results = []

    for case in test_cases:
        print(f"RUNNING TEST: {case['name']}")
        print(f"QUERY: {case['query']}")
        
        try:
            response = orchestrator.pydantic_ai_pipeline(UserQueryInput(query_text=case['query']))
            
            # Basic validation
            passed_intent = response.intent == case['expected_intent']
            passed_content = len(response.evidence_points) > 0 or response.retrieved_case_count == 0
            passed_compliance = "Standard AI disclaimer" in response.compliance_disclaimer or "Generated by AI" in response.compliance_disclaimer
            
            test_result = {
                "name": case['name'],
                "intent_match": passed_intent,
                "has_evidence": len(response.evidence_points) > 0,
                "summary_length": len(response.summary),
                "status": "PASSED" if passed_intent and passed_content else "PEAR_REVIEW_NEEDED"
            }
            results.append(test_result)
            
            print(f"RESULT: {test_result['status']}")
            print(f"INTENT: {response.intent}")
            print(f"SUMMARY: {response.summary[:100]}...")
            print("-" * 40 + "\n")
            
        except Exception as e:
            print(f"FAILED: {str(e)}\n")
            results.append({"name": case['name'], "status": "FAILED", "error": str(e)})

    # Final Summary Report
    print("="*80)
    print("FINAL TEST SUMMARY")
    print("="*80)
    for res in results:
        status_icon = "ðŸŸ¢" if res['status'] == "PASSED" else "ðŸ”´"
        print(f"{status_icon} {res['name']}: {res['status']}")
    print("="*80 + "\n")

if __name__ == "__main__":
    # Ensure working directory is the library root
    run_quality_tests()
